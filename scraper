#!/usr/bin/env -S uv run --script
#
# Description: opens a webpage, reads the content, cleans it up, and copies to clipboard
# Usage scraper <url>
#
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "crawl4ai",
#     "pyperclip",
# ]
# ///

import sys
import asyncio
from crawl4ai import *
import pyperclip

def get_script_name():
    return sys.argv[0].rsplit("/", 1)[-1].rsplit("\\", 1)[-1].replace(".py", "")

def get_url():
    if len(sys.argv) < 2:
        print(f"Usage: python {get_script_name()} <url>")
        sys.exit(1)
    return sys.argv[1]

async def scrape(url):
    md_generator = DefaultMarkdownGenerator(
        content_filter=filter,
        options={"ignore_links": True}
    )
    run_config = CrawlerRunConfig(
        markdown_generator=md_generator,
        excluded_tags=['a', 'form', 'header', 'aside', 'nav', 'footer'],
        word_count_threshold=10,
        cache_mode=CacheMode.BYPASS
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=run_config)
    return result.markdown

async def main():
    url = get_url()
    markdown = await scrape(url)
    pyperclip.copy(markdown)
    print("Copied content of page to clipboard.")

if __name__ == "__main__":
    asyncio.run(main())